{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwokModel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "R4lXW0DUjSx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ]
    },
    {
      "metadata": {
        "id": "KuRLQhQGjF7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qskHQzIOjXv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e52bc34b-2b7f-4fdb-ab16-ea891e73a18b"
      },
      "cell_type": "code",
      "source": [
        "train_dataset = dsets.MNIST(root='/data', train=True, transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "# Download MNIST dataset, make it trainable, convert to Tensor"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8221079.01it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 134876.57it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2224147.22it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 51391.80it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9yo5Zwqzjr7W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "ug-JamIrjWsz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Loading MNIST Test Dataset**\n"
      ]
    },
    {
      "metadata": {
        "id": "MK9AtJkrjtBE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset = dsets.MNIST(root='/data', train=False, transform = transforms.ToTensor(), download = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wExiFZ1Zjxey",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Make Dataset Iterable**"
      ]
    },
    {
      "metadata": {
        "id": "K4LjjhqLjzls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "num_epochs\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rr3iAlMzj908",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Building Model**"
      ]
    },
    {
      "metadata": {
        "id": "Aqnd_IfEj_rO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class neuralnet(nn.Module):\n",
        "  def __init__(self,input_dim,hidden_dim,output_dim):\n",
        "    super(neuralnet, self).__init__()\n",
        "    # Linear Function\n",
        "    self.f1 = nn.Linear(input_dim, hidden_dim)\n",
        "    \n",
        "    # Non-Linear Function\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    # Linear Function (readout)\n",
        "    self.f2 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    # Linear Function\n",
        "    out = self.f1(x)\n",
        "    # Non-Linear Function\n",
        "    out = self.sigmoid(out)\n",
        "    # Linear Function (readout)\n",
        "    out = self.f2(out)\n",
        "    return out\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-stC1-Eomutq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Input Dimension:\n",
        "Size of Image = 28 x 28 = 784\n",
        "\n",
        "Hidden Dimension = 100\n",
        "\n",
        "Output Dimension\n",
        "= 10\n",
        "[0,1,2,3,4,5,6,7,8,9]\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "b4shG7BQm6NJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = neuralnet(input_dim, hidden_dim, output_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hgUtwThbnqHs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Instantiate Loss Class**\n",
        "\n",
        "**Cross Entropy Loss**"
      ]
    },
    {
      "metadata": {
        "id": "8utnD9ZAnsxK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lossfunc = nn.CrossEntropyLoss()\n",
        "\n",
        "# CrossEntropyLoss() will compute softmax(logistic/softmax functions)\n",
        "# and compute cross entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MgeNIO1lnyd0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Instantiate Optimizer Class**"
      ]
    },
    {
      "metadata": {
        "id": "Ul4K_A6vnzXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5gll2jr2n4K-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Parameters**"
      ]
    },
    {
      "metadata": {
        "id": "nLY5kXKbn8gf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a397e270-4b74-4c8e-e843-35e5f84531f8"
      },
      "cell_type": "code",
      "source": [
        "print(model.parameters())\n",
        "\n",
        "print(len(list(model.parameters())))\n",
        "\n",
        "# Hidden Layer Parameters\n",
        "print(list(model.parameters())[0].size())\n",
        "\n",
        "# FC 1 Bias Parameters\n",
        "print(list(model.parameters())[1].size())\n",
        "\n",
        "# FC 2 Parameters\n",
        "print(list(model.parameters())[2].size())\n",
        "\n",
        "# FC 2 Bias Parameters\n",
        "print(list(model.parameters())[3].size())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object Module.parameters at 0x7fc18c4c9db0>\n",
            "4\n",
            "torch.Size([100, 784])\n",
            "torch.Size([100])\n",
            "torch.Size([10, 100])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a35_gkMGtCEU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Train Model**"
      ]
    },
    {
      "metadata": {
        "id": "YlyiGzWytCuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e8babca6-bb60-4507-b906-89f4bb01f07e"
      },
      "cell_type": "code",
      "source": [
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # Load images as variable\n",
        "    images = Variable(images.view(-1,28*28))\n",
        "    labels = Variable(labels)\n",
        "                      \n",
        "    # Clear gradients\n",
        "    optimizer.zero_grad()\n",
        "                      \n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "                      \n",
        "    # Calculate Loss: softmax -> cross entropy loss\n",
        "    loss = lossfunc(outputs, labels)\n",
        "                      \n",
        "    # Back prop\n",
        "    loss.backward()\n",
        "    \n",
        "    # Updating Parameters\n",
        "    optimizer.step()\n",
        "                      \n",
        "    \n",
        "    iter+=1\n",
        "         \n",
        "    if iter%500 == 0:\n",
        "      # Calculate Accuracy\n",
        "      correct = 0\n",
        "      total = 0    \n",
        "      # Iterate through test dataset\n",
        "      for images, labels in test_loader:\n",
        "        # Load images to a torch variable\n",
        "        images = Variable(images.view(-1,28*28))\n",
        "           \n",
        "        # Forward pass to get logits/output\n",
        "        outputs = model(images)\n",
        "                      \n",
        "        # Get predictions from max value\n",
        "        _, predicted = torch.max(outputs.data,1)\n",
        "                      \n",
        "        # Total number of labels\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        # Total correct predictions\n",
        "        correct += (predicted == labels).sum()\n",
        "        \n",
        "       \n",
        "      accuracy = 100 * correct/total\n",
        "      \n",
        "      # Print Loss\n",
        "      print('Iterations: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))\n",
        "                      \n",
        "                      "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations: 500. Loss: 0.5082769393920898. Accuracy: 86\n",
            "Iterations: 1000. Loss: 0.4441291093826294. Accuracy: 89\n",
            "Iterations: 1500. Loss: 0.4148089289665222. Accuracy: 90\n",
            "Iterations: 2000. Loss: 0.2600734233856201. Accuracy: 91\n",
            "Iterations: 2500. Loss: 0.3360753655433655. Accuracy: 91\n",
            "Iterations: 3000. Loss: 0.2839522659778595. Accuracy: 92\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}